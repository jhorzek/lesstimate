{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"lesstimate","text":"<p>lesstimate (lesstimate estimates sparse estimates) is a C++ header-only library that lets you combine statistical models such linear regression with state of the art penalty functions (e.g., lasso, elastic net, scad). With lesstimate you can add regularization and variable selection procedures to your existing modeling framework. It is currently used in lessSEM to regularize structural equation models.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multiple penalty functions: lesstimate lets you apply any of the following penalties: ridge, lasso, adaptive lasso, elastic net, cappedL1, lsp, scad, mcp. Furthermore, you can combine multiple penalties.</li> <li>State of the art optimizers: lesstimate provides two state of the art optimizers--variants of glmnet and ista.</li> <li>Header-only: lesstimate is designed as a header-only library. Include the headers and you are ready to go.</li> <li>Builds on armadillo: lesstimate builds on the popular C++ armadillo library, providing you with access to a wide range of mathematical functions to create your model.</li> <li>R and C++: lesstimate can be used in both, R and C++ libraries.</li> </ul>"},{"location":"#details","title":"Details","text":"<p>lesstimate lets you optimize fitting functions of the form</p> <p>$$g(\\pmb\\theta) = f(\\pmb\\theta) + p(\\pmb\\theta),$$</p> <p>where $f(\\pmb\\theta)$ is a smooth objective function (e.g., residual sum squared, weighted least squares or log-Likelihood) and $p(\\pmb\\theta)$ is a non-smooth penalty function (e.g., lasso or scad).</p> <p>To use the optimziers, you will need two functions:</p> <ol> <li>a function that computes the fit value $f(\\pmb\\theta)$ of your model</li> <li>(optional) a functions that computes the gradients $\\triangledown_{\\pmb\\theta}f(\\pmb\\theta)$ of the model. If none is provided, the gradients will be approximated numerically, which can be quite slow</li> </ol> <p>Given these two functions, lesstimate lets you apply any of the aforementioned penalties with the quasi-Newton glmnet optimizer developed by Friedman et al. (2010) and Yuan et al. (2012) or variants of the proximal-operator based ista optimizer (see e.g., Gong et al., 2013). Because both optimziers provide a very similar interface, switching between them is fairly simple. This interface is inspired by the ensmallen library. </p> <p>lesstimate was mainly developed to be used in lessSEM, an  R package for regularized Structural Equation Models. However, the library can also be used from C++. lesstimate builds heavily on the RcppAdmadillo (Eddelbuettel et al., 2014) and armadillo (Sanderson et al., 2016)  libraries. The optimizer interface is inspired by the ensmallen library (Curtin et al., 2021).</p>"},{"location":"#references","title":"References","text":""},{"location":"#software","title":"Software","text":"<ul> <li>Curtin R R, Edel M, Prabhu R G, Basak S, Lou Z, Sanderson C (2021). The ensmallen library for flexible numerical optimization. Journal of Machine Learning Research, 22 (166).</li> <li>Eddelbuettel D, Sanderson C (2014). \u201cRcppArmadillo: Accelerating R with high-performance C++ linear algebra.\u201d Computational Statistics and Data Analysis, 71, 1054\u20131063. doi:10.1016/j.csda.2013.02.005.</li> <li>Sanderson C, Curtin R (2016). Armadillo: a template-based C++ library for linear algebra. Journal of Open Source Software, 1 (2), pp. 26.</li> </ul>"},{"location":"#penalty-functions","title":"Penalty Functions","text":"<ul> <li>Cand\u00e8s, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity   by Reweighted l1 Minimization. Journal of Fourier Analysis and   Applications, 14(5\u20136), 877\u2013905.   https://doi.org/10.1007/s00041-008-9045-x</li> <li>Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized   likelihood and its oracle properties. Journal of the American   Statistical Association, 96(456), 1348\u20131360.   https://doi.org/10.1198/016214501753382273</li> <li>Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased   Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55\u201367.   https://doi.org/10.1080/00401706.1970.10488634</li> <li>Tibshirani, R. (1996). Regression shrinkage and selection via the   lasso. Journal of the Royal Statistical Society. Series B   (Methodological), 58(1), 267\u2013288.</li> <li>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax   concave penalty. The Annals of Statistics, 38(2), 894\u2013942.   https://doi.org/10.1214/09-AOS729</li> <li>Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse   Regularization. Journal of Machine Learning Research, 11, 1081\u20131107.</li> <li>Zou, H. (2006). The adaptive lasso and its oracle properties. Journal   of the American Statistical Association, 101(476), 1418\u20131429.   https://doi.org/10.1198/016214506000000735</li> <li>Zou, H., &amp; Hastie, T. (2005). Regularization and variable selection   via the elastic net. Journal of the Royal Statistical Society: Series   B, 67(2), 301\u2013320. https://doi.org/10.1111/j.1467-9868.2005.00503.x</li> </ul>"},{"location":"#glmnet","title":"GLMNET","text":"<ul> <li>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1\u201320. https://doi.org/10.18637/jss.v033.i01</li> <li>Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421</li> </ul>"},{"location":"#variants-of-ista","title":"Variants of ISTA","text":"<ul> <li>Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1), 183\u2013202. https://doi.org/10.1137/080716542</li> <li>Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013). A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. Proceedings of the 30th International Conference on Machine Learning, 28(2)(2), 37\u201345.</li> <li>Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and Trends in Optimization, 1(3), 123\u2013231.</li> </ul>"},{"location":"01-installation/","title":"Installation","text":"<p>Being a header-only library, lesstimate is fairly easy to install. However, depending on your use-case, the dependencies of lesstimate have  to be installed differently.</p>"},{"location":"01-installation/#using-lesstimate-in-your-r-package","title":"Using lesstimate in your R package","text":"<p>We provide a package template using lesstimate. All procedures outlined in the following are already implemented in this template.</p> <p>When using lesstimate in your R package, first make sure that RcppArmadillo (Eddelbuettel et al., 2014) is installed (if not, run <code>install.packages(\"RcppArmadillo\")</code>. You will also need a C++ compiler. Instructions to install C++  can be found in the Advanced R book by Hadley Wickham.</p> <p>Next, create a folder called <code>inst</code> in your R package. Within <code>inst</code>, create a folder called <code>include</code>. You should now have the following folder structure:</p> <pre><code>|- R\n|- inst\n|    |- include\n|- src\n|    |- Makevars\n|    |- Makevars.win\n|- DESCRIPTION\n|- .Rbuildignore\n</code></pre> <p>Clone the lesstimate git repository in your include folder with <code>git clone https://github.com/jhorzek/lesstimate.git</code>.</p> <p>Add a file called <code>lessSEM.h</code> in the folder inst/include (see here for an example) and save the following in this file:</p> <pre><code>#ifndef lessSEM_H\n#define lessSEM_H\n\n#include \"lesstimate/include/lesstimate.h\"\n\n#endif\n</code></pre> <p>This file makes sure that R can find the functions of lesstimate. </p> <p>In your src-folder, open the files Makevars and Makevars.win. Add the following line (see here): <pre><code>PKG_CXXFLAGS = $(SHLIB_OPENMP_CXXFLAGS) -I../inst/include/\n</code></pre> This ensures that the include folder is found when compiling the package.</p> <p>Next add <code>^lesstimateConfig-cmake$</code> to .Rbuildignore.</p> <p>Open the DESCRIPTION file of your package and add <code>Rcpp</code> and <code>RcppArmadillo</code> to the field <code>LinkingTo</code> (see here).</p>"},{"location":"01-installation/#using-lesstimate-in-your-c-package","title":"Using lesstimate in your C++ package","text":"<p>We provide a template using lesstimate with C++. All procedures outlined in the following are already implemented in this template. We  highly recommend that you use this template as a starting point.</p> <p>For C++, you will need the armadillo - library (Sanderson et al., 2016). lesstimate uses the cpm.Cmake package manager to handle its dependencies. However, not all dependencies will be installed automatically. armadillo requires BLAS and LAPACK, both of which have to be installed on the system. Instructions to install dependencies of armadillo are provided in the documentation of the package. </p>"},{"location":"01-installation/#references","title":"References","text":"<ul> <li>Eddelbuettel D, Sanderson C (2014). \u201cRcppArmadillo: Accelerating R with high-performance C++ linear algebra.\u201d Computational Statistics and Data Analysis, 71, 1054\u20131063. doi:10.1016/j.csda.2013.02.005.</li> <li>Sanderson C, Curtin R (2016). Armadillo: a template-based C++ library for linear algebra. Journal of Open Source Software, 1 (2), pp. 26.</li> </ul>"},{"location":"02-getting-started/","title":"Getting Started","text":"RC++ <p>All steps outlined in the following are provided in the template for R packages using lesstimate. The C++ code can be found in the src directory of the package.</p> <p>All steps outlined in the following are provided in the template for C++ packages using lesstimate. The C++ code can be found in the linear_regression.cpp file of the package.</p> <p>lesstimate was initially a sub-folder of the lessSEM package. Therefore, the default is currently to still assume that you are using the library in an R package. In the common_headers.h-file,  you will find a variable called <code>USE_R</code>. If this variable is set to 1 (default), lesstimate is setup to be used from R. If <code>USE_R</code> is set to zero, the library no longer relies on the R packages Rcpp (Eddelbuettel et al., 2011) or RcppArmadillo (Eddelbuettel et al., 2014). The library can now be used in  C++ projects as long as the armadillo (Sanderson et al., 2016) library is installed. </p>"},{"location":"02-getting-started/#interfacing-to-lesstimate","title":"Interfacing to lesstimate","text":"<p>As outlined in the introduction, the optimizers in lesstimate minimize functions of the form  $$g(\\pmb\\theta) = f(\\pmb\\theta) + p(\\pmb\\theta)$$ where $f(\\pmb\\theta)$ is (twice) continously differentiable with respect to $\\theta$ and $p(\\pmb\\theta)$ is a non-differentiable penalty function (e.g., lasso or scad). To use lesstimate for your project, you will need two functions. First, a function that computes $f(\\pmb\\theta)$, the value of your un- regularized objective function. Second, a function that computes  $\\triangledown_{\\pmb\\theta} f(\\pmb\\theta)$, the gradient vector of your un-regularized objective function with respect to the parameters $\\pmb\\theta$. We also assume that these functions use armadillo. If you don't use armadillo, you may have to write a translation layer. The optimizer interface outlined in the following is adapted from the ensmallen library (Curtin et al.; 2021).</p>"},{"location":"02-getting-started/#step-1-including-the-necessary-headers","title":"Step 1: Including the necessary headers","text":"<p>First, let's include the necessary headers:</p> RC++ <pre><code>#include &lt;RcppArmadillo.h&gt;\n// [[Rcpp :: depends ( RcppArmadillo )]]\n</code></pre> <pre><code>#include &lt;armadillo&gt;\n</code></pre>"},{"location":"02-getting-started/#step-2-implement-the-fit-and-gradient-functions","title":"Step 2: Implement the fit and gradient functions:","text":"<p>Let's assume that you want to estimate a linear regression model. In this case, the fit and gradient function could look as follows:</p> <pre><code>double sumSquaredError(\n    arma::colvec b, // the parameter vector\n    arma::colvec y, // the dependent variable\n    arma::mat X     // the design matrix\n)\n{\n  // compute the sum of squared errors:\n  arma::mat sse = arma::trans(y - X * b) * (y - X * b);\n\n  // other packages, such as glmnet, scale the sse with\n  // 1/(2*N), where N is the sample size. We will do that here as well\n\n  sse *= 1.0 / (2.0 * y.n_elem);\n\n  // note: We must return a double, but the sse is a matrix\n  // To get a double, just return the single value that is in\n  // this matrix:\n  return (sse(0, 0));\n}\n\narma::rowvec sumSquaredErrorGradients(\n    arma::colvec b, // the parameter vector\n    arma::colvec y, // the dependent variable\n    arma::mat X     // the design matrix\n)\n{\n  // note: we want to return our gradients as row-vector; therefore,\n  // we have to transpose the resulting column-vector:\n  arma::rowvec gradients = arma::trans(-2.0 * X.t() * y + 2.0 * X.t() * X * b);\n\n  // other packages, such as glmnet, scale the sse with\n  // 1/(2*N), where N is the sample size. We will do that here as well\n\n  gradients *= (.5 / y.n_rows);\n\n  return (gradients);\n }\n</code></pre> <p>With these two functions, we are ready to go. If you want to use the glmnet optimizer, you may want to also implement a function that computes the Hessian. It can be beneficial to provide a good starting point for the bfgs updates using this Hessian.</p>"},{"location":"02-getting-started/#step-3-creating-a-model-object","title":"Step 3: Creating a model object","text":"<p>lesstimate assumes that you pass a model-object to the optimizers. This model  object ist implemented in the <code>less::model</code>-class. Therefore, we have to create a custom class that inherits from <code>less::model</code> and  implements our linear regression using the functions defined above. </p> RC++ <p>Make sure to follow the install instructions for R first to make this work: <pre><code>#include \"lessSEM.h\"\n</code></pre></p> <p>We recommend setting up lesstimate with CMake. You will find an example here. <pre><code>#include &lt;lesstimate.h&gt;\n</code></pre></p> <pre><code>// IMPORTANT: The library is called lesstimate, but\n// because it was initially a sub-folder of lessSEM, there\n// are two namespaces that are identical: less and lessSEM.\n\nclass linearRegressionModel : public less::model\n{\n\npublic:\n  // the less::model class has two methods: \"fit\" and \"gradients\".\n  // Both of these methods must follow a fairly strict framework.\n  // First: They must receive exactly two arguments:\n  //        1) an arma::rowvec with current parameter values\n  //        2) an Rcpp::StringVector with current parameter labels\n  //          (NOTE: the lessSEM package currently does not make use of these\n  //          labels. This is just for future use. If you don't want to use \n  //          the labels, just pass any less::stringVector you want).\n  //          if you are using R, a less::stringVector is just an \n  //          Rcpp::StringVector. Otherwise it is a custom vector. that can\n  //          be created with less::stringVector myVector(numberofParameters).\n  // Second:\n  //        1) fit must return a double (e.g., the -2-log-likelihood)\n  //        2) gradients must return an arma::rowvec with the gradients. It is\n  //           important that the gradients are returned in the same order as the\n  //           parameters (i.e., don't shuffle your gradients, lessSEM will \n  //           assume that the first value in gradients corresponds to the\n  //           derivative with respect to the first parameter passed to \n  //           the function).\n\n  double fit(arma::rowvec b, less::stringVector labels) override\n  {\n    // NOTE: In sumSquaredError we assumed that b was a column-vector. We\n    //  have to transpose b to make things work\n    return (sumSquaredError(b.t(), y, X));\n  }\n\n  arma::rowvec gradients(arma::rowvec b, less::stringVector labels) override\n  {\n    // NOTE: In sumSquaredErrorGradients we assumed that b was a column-vector. We\n    //  have to transpose b to make things work\n    return (sumSquaredErrorGradients(b.t(), y, X));\n  }\n\n  // IMPORTANT: Note that we used some arguments above which we did not pass to\n  // the functions: y, and X. Without these arguments, we cannot use our\n  // sumSquaredError and sumSquaredErrorGradients function! To make these \n  // accessible to our functions, we have to define them:\n\n  const arma::colvec y;\n  const arma::mat X;\n\n  // finally, we create a constructor for our class\n  linearRegressionModel(arma::colvec y_, arma::mat X_) : y(y_), X(X_){};\n};\n</code></pre> <p>Instances of <code>linearRegressionModel</code> can be passed to the <code>glmnet</code> or <code>ista</code> optimizers.</p>"},{"location":"02-getting-started/#step-4-interfacing-to-the-optimizers","title":"Step 4: Interfacing to the optimizers","text":"<p>Ther are two interfaces you can use: </p> <ol> <li>A specialized interface, where the model is penalized only using one specific penalty function. This requires more work, but is typically a bit faster than using the second approach.</li> <li>A simplified interface that allows you to use any of the penalty functions (and also mix them)</li> </ol> <p>We will use the simplified interface in the following. To this end, we will first create a new instance of our linearRegressionModel:</p> <pre><code>arma::mat X = {{1.00, -0.70, -0.86},\n               {1.00, -1.20, -2.10},\n               {1.00, -0.15,  1.13},\n               {1.00, -0.50, -1.50},\n               {1.00,  0.83,  0.44},\n               {1.00, -1.52, -0.72},\n               {1.00,  1.40, -1.30},\n               {1.00, -0.60, -0.59},\n               {1.00, -1.10,  2.00},\n               {1.00, -0.96, -0.20}};\n\narma::colvec y = {{ 0.56},\n                  {-0.32},\n                  { 0.01},\n                  {-0.09},\n                  { 0.18},\n                  {-0.11},\n                  { 0.62},\n                  { 0.72},\n                  { 0.52},\n                  { 0.12}};\n\nlinearRegressionModel linReg(y, X);\n</code></pre> <p>Next, we create a vector with starting values using armadillo. This vector must be of  length of the number of parameters in the model. In our case, these are three: the intercept and two predictors.</p> <p><pre><code>arma::rowvec startingValues(3);\nstartingValues.fill(0.0);\n</code></pre> lesstimate also expects labels for these parameters. The labels are stored in an object of class <code>less::stringVector</code>:</p> RC++ <p>When using R, <code>less::stringVector</code> is identical to <code>Rcpp::StringVector</code>. <pre><code>Rcpp::StringVector parameterLabels(3);\nparameterLabels[0] = \"b0\";\nparameterLabels[1] = \"b1\";\nparameterLabels[2] = \"b2\";\n</code></pre></p> <pre><code>std::vector&lt;std::string&gt; labels {\"b0\", \"b1\", \"b2\"};\nless::stringVector parameterLabels(labels);\n</code></pre> <p>Now we have to specify the values for our tuning parameters. These depend on the penalty we want to use:</p> <p></p> <p>Note that some penalty functions only have $\\lambda$ as tuning parameter, while others have $\\lambda$ and $\\theta$. The interface used below does not support elastic net penalties; that is, $\\alpha$ is not used. Furthermore, the adaptive lasso requires setting up the weights manually by specifying parameter-specific $\\lambda$-values. We now specify for each parameter,  which penalty we want to use and what values the tuning parameter should have for this parameter.  If a tuning parameter is not used by the respective penalty, just set it to 0 or any other value:</p> lassomixed penalty <pre><code>// penalty: We don't penalize the intercept b0, but we penalize\n// b1 and b2 with lasso:\nstd::vector&lt;std::string&gt; penalty{\"none\", \"lasso\", \"lasso\"};\n\n// tuning parameter lambda:\narma::rowvec lambda = {{0.0, 0.2, 0.2}};\n// theta is not used by the lasso penalty:\narma::rowvec theta = {{0.0, 0.0, 0.0}};\n</code></pre> <pre><code>// penalty: We don't penalize the intercept b0, but we penalize\n// b1 with lasso and b2 with scad:\nstd::vector&lt;std::string&gt; penalty{\"none\", \"lasso\", \"scad\"};\n\n// tuning parameter lambda:\narma::rowvec lambda = {{0.0, 0.2, 0.1}};\n// theta is not used by the lasso penalty:\narma::rowvec theta = {{0.0, 0.0, 3.6}};\n</code></pre> <p>Note that the penalty used for each parameter is specified in the <code>std::vector&lt;std::string&gt;</code> vector <code>penalty</code>. Currently, any of the following penalty labels is supported:  <code>\"none\"</code> (no penalty), <code>\"cappedL1\"</code>, <code>\"lasso\"</code>,  <code>\"lsp\"</code>, <code>\"mcp\"</code>, and <code>\"scad\"</code>. </p> <p>Having specified the penalty and the tuning values, we can now use the optimizers:</p> glmnetista <p>To use glmnet successfully, you should also provide an initial Hessian as this can  improve the optimization considerably. For simplicity, we won't do this here.</p> <pre><code>less::fitResults fitResult_ = less::fitGlmnet(\n      linReg,\n      startingValues,\n      parameterLabels,\n      penalty,\n      lambda,\n      theta//,\n      // initialHessian, // optional, but can be very useful\n    // controlOptimizer, // optional: change the settings of the optimizer.\n    // verbose // set to &gt;0 to get additional information on the optimization\n  );\n</code></pre> <pre><code>less::fitResults fitResult_ = less::fitIsta(\n    linReg,\n    startingValues,\n    parameterLabels,\n    penalty,\n    lambda,\n    theta//,\n    // controlOptimizer, // optional: change the settings of the optimizer\n    // verbose // set to &gt;0 to get additional information on the optimization\n    );\n</code></pre> <p>In the <code>fitResults_</code> object, you will find:</p> <ul> <li>fit: the final fit value (regularized fit)</li> <li>fits: a vector with all fits at the outer iteration</li> <li>convergence: was the outer breaking condition met?</li> <li>parameterValues: final parameter values (of class <code>arma::rowvec</code>)</li> <li>Hessian: final Hessian approximation in case of glmnet (Note: this is a bfgs approximated Hessian). Returning this Hessian is useful because it can be used as input for the next optimization if glmnet is used with multiple tuning parameter settings (e.g., $\\lambda \\in {0, .1, .2, ..., 1}$.</li> </ul>"},{"location":"02-getting-started/#optimizer-settings","title":"Optimizer settings","text":"<p>The default settings of the optimizers may not work for your use case. Adapting these settings can therefore be crucial for a  succesful optimization. In the <code>fitGlmnet</code> and <code>fitIsta</code> functions above, the optimizer settings are adapted using the <code>controlOptimizer</code> argument. Depending on the optimizer used, different settings can be adapted.</p> glmnetista <p>The glmnet optimizer has the following additional settings:</p> <ul> <li><code>initialHessian</code>: an <code>arma::mat</code> with the initial Hessian matrix fo the optimizer. In case of the simplified interface, this  argument should not be used. Instead, pass the initial Hessian as shown above</li> <li><code>stepSize</code>: a <code>double</code> specifying the initial stepSize of the outer iteration ($\\theta_{k+1} = \\theta_k + \\text{stepSize} * \\text{stepDirection}$)</li> <li><code>sigma</code>: a <code>double</code> that is only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421.</li> <li><code>gamma</code>: a <code>double</code> controling the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.</li> <li><code>maxIterOut</code>: an <code>int</code> specifying the maximal number of outer iterations</li> <li><code>maxIterIn</code>: an <code>int</code> specifying the maximal number of inner iterations</li> <li><code>maxIterLine</code>: an <code>int</code> specifying the maximal number of iterations for the line search procedure</li> <li><code>breakOuter</code>: a <code>double</code> specyfing the stopping criterion for outer iterations</li> <li><code>breakInner</code>: a <code>double</code> specyfing the stopping criterion for inner iterations</li> <li><code>convergenceCriterion</code>: a <code>convergenceCriteriaGlmnet</code> specifying which convergence criterion should be used for the outer iterations. Possible are <code>less::GLMNET</code>, <code>less::fitChange</code>, and <code>less::gradients</code>. </li> <li><code>verbose</code>: an <code>int</code>, where 0 prints no additional information, &gt; 0 prints GLMNET iterations</li> </ul> <pre><code>// First, create a new instance of class controlGLMNET:\nless::controlGLMNET controlOptimizer = less::controlGlmnetDefault();\n// Next, adapt the settings:\ncontrolOptimizer.maxIterOut = 1000;\n// pass the argument to the fitGlmnet function:\nless::fitResults fitResult_ = less::fitGlmnet(\n      linReg,\n      startingValues,\n      parameterLabels,\n      penalty,\n      lambda,\n      theta,\n     // sets Hessian to identity; a better Hessian will help!\n      arma::mat(1, 1, arma::fill::ones), // initial Hessian\n    controlOptimizer//,\n    // verbose // set to &gt;0 to get additional information on the optimization\n  );\n</code></pre> <p>The ista optimizer has the following additional settings:</p> <ul> <li><code>L0</code>: a <code>double</code> controling the step size used in the first iteration</li> <li><code>eta</code>: a <code>double</code> controling by how much the step size changes in inner iterations with $(\\eta^i)*L$, where $i$ is the inner iteration</li> <li><code>accelerate</code>: a <code>bool</code>; if  the extrapolation parameter is used to accelerate ista (see, e.g., Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms.  Foundations and Trends in Optimization, 1(3), 123\u2013231., p. 152)</li> <li><code>maxIterOut</code>: an <code>int</code> specifying the maximal number of outer iterations</li> <li><code>maxIterIn</code>: an <code>int</code> specifying the maximal number of inner iterations</li> <li><code>breakOuter</code>: a <code>double</code> specyfing the stopping criterion for outer iterations</li> <li><code>breakInner</code>: a <code>double</code> specyfing the stopping criterion for inner iterations</li> <li><code>convCritInner</code>: a <code>convCritInnerIsta</code> that specifies the inner breaking condition. Can be set to <code>less::istaCrit</code> (see Beck &amp; Teboulle (2009);  Remark 3.1 on p. 191 (ISTA with backtracking)) or <code>less::gistCrit</code> (see Gong et al., 2013; Equation 3) </li> <li><code>sigma</code>: a <code>double</code> in (0,1) that is used by the gist convergence criterion. Larger sigma enforce larger improvement in fit</li> <li><code>stepSizeIn</code>: a <code>stepSizeInheritance</code> that specifies how step sizes should be carried forward from iteration to iteration. <code>less::initial</code>: resets the step size to L0 in each iteration, <code>less::istaStepInheritance</code>: takes the previous step size as initial value for the next iteration, <code>less::barzilaiBorwein</code>: uses the Barzilai-Borwein procedure, <code>less::stochasticBarzilaiBorwein</code>: uses the Barzilai-Borwein procedure, but sometimes resets the step size; this can help when the optimizer is caught in a bad spot.</li> <li><code>sampleSize</code>: an <code>int</code> that can be used to scale the fitting function down if the fitting function depends on the sample size</li> <li><code>verbose</code>: an <code>int</code>, where 0 prints no additional information, &gt; 0 prints GLMNET iterations</li> </ul> <pre><code>// First, create a new instance of class controlIsta:\nless::controlIsta controlOptimizer = less::controlIstaDefault();\n// Next, adapt the settings:\ncontrolOptimizer.maxIterOut = 1000;\n// pass the argument to the fitIsta function:\nless::fitResults fitResult_ = less::fitIsta(\n    linReg,\n    startingValues,\n    parameterLabels,\n    penalty,\n    lambda,\n    theta,\n    controlOptimizer//,\n    // verbose // set to &gt;0 to get additional information on the optimization\n  );\n</code></pre>"},{"location":"02-getting-started/#specialized-interfaces","title":"Specialized interfaces","text":"<p>If you are only interested in one specific penalty function (e.g., lasso), it can be beneficial to use the specialized interfaces provided by lesstimate. These can be faster because lesstimate no longer has to check which penalty is used for which parameter. That said, the specialized interface takes  some more time to set up and is less flexible than the simplified interface used above.</p> <p>To use the specialized interface, we have to define the penalties we want to use. In general, the specialized interface allows for specifying two penalties for each parameter: a smooth (i.e., differentiable)  penalty (currently only ridge is supported) and a non-smooth (i.e., non-differentiable) penalty (any of the penalties mentioned above). We will use  the elastic net below as this penalty combines a ridge and a lasso penalty.</p> glmnetista <pre><code>// Specify the penalties we want to use:\nless::penaltyLASSOGlmnet lasso;\nless::penaltyRidgeGlmnet ridge;\n// Note that we used the glmnet variants of lasso and ridge. The reason\n// for this is that the glmnet implementation allows for parameter-specific\n// lambda and alpha values while the current ista implementation does not.\n\n// These penalties take tuning parameters of class tuningParametersEnetGlmnet\nless::tuningParametersEnetGlmnet tp;\n\n// We have to specify alpha and lambda. Here, different values can \n// be specified for each parameter:\ntp.lambda = arma::rowvec(startingValues.n_elem);\ntp.lambda.fill(0.2);\ntp.alpha = arma::rowvec(startingValues.n_elem);\ntp.alpha.fill(0.3); \n\n// Finally, there is also the weights. The weights vector indicates, which\n// of the parameters is regularized (weight = 1) and which is unregularized \n// (weight =0). It also allows for adaptive lasso weights (e.g., weight =.0123).\n// weights must be an arma::rowvec of the same length as our parameter vector.\narma::rowvec weights(startingValues.n_elem);\nweights.fill(1.0); // we want to regularize all parameters\nweights.at(0) = 0.0; // except for the first one, which is our intercept.\ntp.weights = weights;   \n\n// to optimize this model, we have to pass it to\n// the glmnet function:\n\nless::fitResults lmFit = less::glmnet(\n    linReg, // the first argument is our model\n    startingValues, // arma::rowvec with starting values\n    parameterLabels, // less::stringVector with labels\n    lasso, // non-smooth penalty\n    ridge, // smooth penalty\n    tp//,    // tuning parameters\n    //controlOptimizer // optional fine-tuning (see above)\n  );\n</code></pre> <pre><code>// The elastic net is a combination of a ridge penalty and \n// a lasso penalty. \n// NOTE: HERE COMES THE BIGGEST DIFFERENCE BETWEEN GLMNET AND ISTA:\n// 1) ISTA ALSO REQUIRES THE DEFINITION OF A PROXIMAL OPERATOR. THESE\n//    ARE CALLED proximalOperatorZZZ IN lessSEM (e.g., proximalOperatorLasso \n//    for lasso).\n// 2) THE SMOOTH PENALTY (RIDGE) AND THE LASSO PENALTY MUST HAVE SEPARATE \n//    TUNING PARMAMETERS.\nless::proximalOperatorLasso proxOp; // HERE, WE DEFINE THE PROXIMAL OPERATOR\nless::penaltyLASSO lasso; \nless::penaltyRidge ridge;\n// BOTH, LASSO AND RIDGE take tuning parameters of class tuningParametersEnet\nless::tuningParametersEnet tpLasso;\nless::tuningParametersEnet tpRidge;\n\n// We have to specify alpha and lambda. Here, the same value is used\n// for each parameter:\ntpLasso.alpha = .3;\ntpLasso.lambda = .2;\ntpRidge.alpha = .3;\ntpRidge.lambda = .2;\n\n// A weights vector indicates, which\n// of the parameters is regularized (weight = 1) and which is unregularized \n// (weight =0). It also allows for adaptive lasso weights (e.g., weight =.0123).\n// weights must be an arma::rowvec of the same length as our parameter vector.\narma::rowvec weights(startingValues.n_elem);\nweights.fill(1.0); // we want to regularize all parameters\nweights.at(0) = 0.0; // except for the first one, which is our intercept.\ntpLasso.weights = weights;\ntpRidge.weights = weights;\n\n// to optimize this model, we have to pass it to the ista function:\n\nless::fitResults lmFit = less::ista(\n  linReg, // the first argument is our model\n  startingValues, // arma::rowvec with starting values\n  parameterLabels, // less::stringVector with labels\n  proxOp, // proximal opertator\n  lasso, // our lasso penalty\n  ridge, // our ridge penalty\n  tpLasso, // our tuning parameter FOR THE LASSO PENALTY\n  tpRidge//, // our tuning parameter FOR THE RIDGE PENALTY\n  //controlOptimizer // optional fine-tuning (see above)\n);\n</code></pre>"},{"location":"02-getting-started/#references","title":"References","text":"<ul> <li>Sanderson C, Curtin R (2016). Armadillo: a template-based C++ library for linear algebra. Journal of Open Source Software, 1 (2), pp. 26.</li> <li>Eddelbuettel D, Fran\u00e7ois R (2011). \u201cRcpp: Seamless R and C++ Integration.\u201d Journal of Statistical Software, 40(8), 1\u201318. doi:10.18637/jss.v040.i08. </li> <li>Eddelbuettel D, Sanderson C (2014). \u201cRcppArmadillo: Accelerating R with high-performance C++ linear algebra.\u201d Computational Statistics and Data Analysis, 71, 1054\u20131063. doi:10.1016/j.csda.2013.02.005. </li> <li>Curtin R R, Edel M, Prabhu R G, Basak S, Lou Z, Sanderson C (2021). The ensmallen library for flexible numerical optimization. Journal of Machine Learning Research, 22 (166).</li> </ul>"},{"location":"03-The%20Optimizer%20Design/","title":"The Optimizer Design","text":"<p>Note: We will not cover the optimizers themselves; here, we recommend reading the following articles:</p> <p>GLMNET:</p> <ul> <li>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 1\u201320. https://doi.org/10.18637/jss.v033.i01</li> <li>Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421</li> </ul> <p>ISTA:</p> <ul> <li>Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1), 183\u2013202. https://doi.org/10.1137/080716542</li> <li>Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013). A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. Proceedings of the 30th International Conference on Machine Learning, 28(2)(2), 37\u201345.</li> <li>Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations and Trends in Optimization, 1(3), 123\u2013231.</li> </ul>"},{"location":"03-The%20Optimizer%20Design/#the-fitting-function","title":"The Fitting Function","text":"<p>The objective of the optimizers is to minimize the fitting function. In both, glmnet and ista we assume that this fitting function is given by a differentiable part and a non-differentiable part. To be more specific, the fitting function is given by:</p> <p>$$f(\\pmb{\\theta}) = l(\\pmb\\theta) + s(\\pmb\\theta,\\pmb{t}_s) + p(\\pmb\\theta,\\pmb{t}_p)$$</p> <p>where $l(\\pmb\\theta)$ is the unregularized fitting function  (e.g., the -2log-likelihood) in the SEMs implemented in lessSEM.  $s(\\pmb\\theta,\\pmb{t}_s)$ is a differentiable (smooth) penalty function  (e.g., a ridge penalty) and $p(\\pmb\\theta,\\pmb{t}_p)$ is a non-differentiable penalty function (e.g., a lasso penalty). All three functions take the parameter estimates $\\pmb\\theta$ as input and return a single value as output. The penalty  functions $s(\\pmb\\theta,\\pmb{t}_s)$ and $p(\\pmb\\theta,\\pmb{t}_p)$ additionally expect vectors with tuning parameters--$\\pmb{t}_s$ in case of the smooth penalty and $\\pmb{t}_p$ in case of the non-differentiable penalty. Thus, in theory both penalty functions can use different tuning parameters. </p> <p>A prototypical example for fitting functions of the form defined above is the  elastic net. Here, </p> <p>$$f(\\pmb{\\theta}) = l(\\pmb\\theta) + (1-\\alpha)\\lambda \\sum_j\\theta_j^2 + \\alpha\\lambda\\sum_j| \\theta_j|$$</p> <p>The elastic net is a combination of a ridge penalty $\\lambda \\sum_j\\theta_j^2$ and a lasso penalty $\\lambda\\sum_j| \\theta_j|$. Note that in this case, both penalties take in the same tuning parameters  ($\\lambda$ and $\\alpha$).</p> <p>With this in mind, we can now have a closer look at the optimization functions. We will start with glmnet (see function glmnet in the file inst/include/glmnet_class.hpp). This function is called as follows:</p> <pre><code>inline less::fitResults glmnet(model&amp; model_, \n                                  numericVector startingValuesRcpp,\n                                  penaltyLASSOGlmnet&amp; penalty_,\n                                  penaltyRidgeGlmnet&amp; smoothPenalty_, \n                                  const tuningParametersEnetGlmnet tuningParameters,\n                                  const controlGLMNET&amp; control_ = controlGlmnetDefault())\n{...}\n</code></pre> <p>The first argument is a model. This model has to be created by you and must  inherit from the less::model class (see lessLM for an example). Most importantly, this model must provide means to get the value of the first  part of the fitting function: $l(\\pmb\\theta)$. It must also provide means to compute the gradients of your fitting function.</p> <p>The next argument are the starting values which are given as an numericVector. If you are using R, this is an Rcpp::NumericVector, in C++ a less::numericVector. This type is chosen because it can have labels  and in its current implementation lesstimate expects that you give  all your startingValues names.</p> <p>Now come the actual penalty functions. The first one is the non-differentiable penalty: the lasso $p(\\pmb\\theta)$. This must be an object of class penaltyLASSOGlmnet which can be created with <code>less::penaltyLASSOGlmnet()</code>'. Next comes the differentiable ridge penalty which must be of class <code>penaltyRidgeGlmnet</code> and can be created with  <code>less::penaltyRidgeGlmnet</code>. </p> <p>Now, the tuning parameters deviate a bit from what we discussed before. We said that the differentiable and the non-differentiable penalty functions will each take their own vector of tuning parameters ($\\pmb t_s$ and $\\pmb t_p$ respectively). Note, however, that the elastic net uses the same tuning parameters for both, ridge and lasso penalty. In the glmnet optimizer we therefore decided to combine both of these into one: the <code>tuningParametersEnetGlmnet</code> struct. <code>tuningParametersEnetGlmnet</code>  has three slots: alpha (to set $\\alpha$), lambda (to set $\\lambda$) and a slot called weights. Now, the weights allow us to switch off the penalty for selected parameters. For instance, in a linear regression we would not want to  penalize the intercept. To this end, the fitting function that is actually implemented internally is given by</p> <p>$$f(\\pmb{\\theta}) = l(\\pmb\\theta) + (1-\\alpha)\\lambda \\sum_j\\omega_j \\theta_j^2 + \\alpha\\lambda\\sum_j\\omega_j| \\theta_j|$$</p> <p>If we set $\\omega_j = 0$ for a specific parameter, this parameter is unregularized. Setting $\\omega_j = 1$ means that parameter $j$ is penalized. $\\omega_j$ can also take any other value (e.g., $\\omega_j = .4123$) which allows for penalties such as the adaptive lasso. Importantly, the weights vector must be of the  same length as <code>startingValuesRcpp</code>. That is, each parameter must have a weight associated with it in the weights vector.</p> <p>Finally, there is the <code>controlGLMNET</code> argument. This argument lets us fine tune the  optimizer. To use the control argument, create a new control object in C++ as follows:</p> <pre><code>less::controlGLMNET myControlObject = less::controlGlmnetDefault();\n</code></pre> <p>Now, you can tweak each element of myControlObject; e.g.,</p> <pre><code>myControlObject.maxIterOut = 100 // only 100 outer iterations\n</code></pre> <p>If you take a closer look at how the two penalty functions are handled within glmnet, you will realize that we basically absorb the differentiable penalty in the  normal fitting function. That is, only the non-differentiable part gets a special treatment, while the differentiable part is simply added to the differntiable $l(\\pmb\\theta)$. To give an example:</p> <pre><code>gradients_kMinus1 = model_.gradients(parameters_kMinus1, parameterLabels) +\n      smoothPenalty_.getGradients(parameters_kMinus1, parameterLabels, tuningParameters); // ridge part\n</code></pre> <p>Note how the gradients of $l(\\pmb\\theta)$ and $s(\\pmb\\theta,\\pmb{t}_s)$ are combined into one.</p>"},{"location":"03-The%20Optimizer%20Design/#the-ista-variants","title":"The ista variants","text":"<p>Besides the glmnet optimizer, we also implemented variants of ista. These are based on the publications mentioned above. The fitting function is again given by </p> <p>$$f(\\pmb{\\theta}) = l(\\pmb\\theta) + s(\\pmb\\theta,\\pmb{t}_s) + p(\\pmb\\theta,\\pmb{t}_p)$$</p> <p>In the  following, we will build a lot on what we've already discussed regarding the  glmnet optimizer above. </p> <p>First, let's have a look at the ista function;</p> <pre><code>template&lt;typename T, typename U&gt; // T is the type of the tuning parameters\ninline less::fitResults ista(\n    model&amp; model_, \n    numericVector startingValuesRcpp,\n    proximalOperator&lt;T&gt;&amp; proximalOperator_, // proximalOperator takes the tuning parameters\n    // as input -&gt; &lt;T&gt;\n    penalty&lt;T&gt;&amp; penalty_, // penalty takes the tuning parameters\n    smoothPenalty&lt;U&gt;&amp; smoothPenalty_, // smoothPenalty takes the smooth tuning parameters\n    // as input -&gt; &lt;U&gt;\n    const T&amp; tuningParameters, // tuning parameters are of type T\n    const U&amp; smoothTuningParameters, // tuning parameters are of type U\n    const control&amp; control_ = controlDefault()\n)\n{...}\n</code></pre> <p>This function is more complicated that the glmnet function discussed above.  But, let's start with the part that stays the same: First, we still have to pass our model to the function. This model must have a fit and a gradients function which return the fit and the gradient respectively. Next, the function again expects us to provide starting values as an numericVector. We will skip the  <code>proximalOperator</code> and the <code>penalty</code> for the moment (these relate to $p(\\pmb\\theta,\\pmb t_p)$) and concentrate on the <code>smoothPenalty</code> first. This is the function $s(\\pmb\\theta,\\pmb t_s)$. In our previous example, we looked at the elastic net penalty, where the smooth penalty is a ridge penalty function. Now, in the ista optimizer, we can also pass in the ridge penalty as a smooth penalty. In fact,  this is exactly what we do when we use ista to fit the elastic net.  This smooth penalty has the tuning parameters $\\pmb t_s$ which are called <code>smoothTuningParameters</code> in the function call. In case of the elastic net, these would again be $\\alpha$ and $\\lambda$ (and the weights vector). Similar to the glmnet procedure outlined above, the differentiable penalty $s(\\pmb\\theta,\\pmb t_s)$ is simply absorbed in the unregularized fitting function $l(\\pmb\\theta)$. </p> <p>Now, for the non-differentiable part $p(\\pmb\\theta,\\pmb p_s)$, the ista optimizer uses so-called proximal operators. The details are beyond the scope here, but Parikh et al. (2013) provide a very good overview of these algorithms. To make things work with ista, we must pass such a proximal operator to the optimizer. Within lesstimate, we have prepared a few of these proximal operators for well-known penalty functions. Additionally, we need a function which returns the actual penalty value. This is the penalty object in the function call. Finally, the  penalty $p(\\pmb\\theta,\\pmb t_p)$ gets its tuning parameters $\\pmb t_p$. This is the <code>tuningParameters</code> object above. To make things more concrete, let's look at the elastic net again. In this case, penalty would be of class <code>less::penaltyLASSO</code> and the proximal operator of type <code>less::proximalOperatorLasso</code>. The tuning parameters would again be $\\alpha$ and $\\lambda$ (and the weights vector).</p> <p>Note that many of the penalty function implemented in lesstimate are typically not combined with a smooth penalty (e.g., scad, mcp, ...). In this case, you must still pass a smoothPenalty object to ista. To this end, we created the <code>less::noSmoothPenalty</code> class which can be used instead of a smooth penalty like ridge. </p> <p>Finally, there is the control argument. This argument lets us fine tune the  optimizer. To use the control argument, create a new control object as follows:</p> <pre><code>less::control myControlObject = less::controlDefault();\n</code></pre> <p>Now, you can tweak each element of myControlObject; e.g.,</p> <pre><code>myControlObject.L0 = .9\n</code></pre>"},{"location":"04-Model/","title":"Model","text":""},{"location":"04-Model/#model-class","title":"model class","text":"<p><code>model</code> is the base class used in every optimizer implemented in lesstimate. The user specified model should inherit from the model class and must implement the two methods defined therein</p>"},{"location":"04-Model/#methods","title":"methods","text":""},{"location":"04-Model/#fit","title":"fit","text":"<p><code>fit</code> takes arguments parameterValues (arma::rowvec) and parameterLabels (stringVector; see common_headers.h) specifying the parameter values and the labels of the paramters. The function should return the fit value (double).</p> <ul> <li>param parameterValues: numericVector with parameter values</li> <li>param parameterLabels: stringVector with parameterLabels</li> <li>return double</li> </ul>"},{"location":"04-Model/#gradients","title":"gradients","text":"<p><code>gradients</code> takes arguments parameterValues(arma::rowvec) and parameterLabels(stringVector; see common_headers.h) specifying the parameter values and the labels of the paramters.The function should return the gradients(arma::rowvec)</p> <ul> <li>param parameterValues: numericVector with parameter values</li> <li>param parameterLabels: stringVector with parameterLabels</li> <li>return arma::rowvec gradients</li> </ul>"},{"location":"05-lesstimate_types/","title":"lesstimate types","text":""},{"location":"05-lesstimate_types/#use_r","title":"USE_R","text":"<p>The USE_R variable allows us to switch between the R implementation and C++ without R. This variable can either be passed to the compiler using -DUSE_R=1 or -DUSE_R=0 or by changing the value below. By default, lesstimate will assume that you are using R and therefor set USE_R=1 if the variable is not defined otherwise.</p>"},{"location":"05-lesstimate_types/#use_r-1","title":"USE_R = 1","text":""},{"location":"05-lesstimate_types/#stringvector","title":"stringVector","text":"<p>Identical to <code>Rcpp::NumericVector</code></p>"},{"location":"05-lesstimate_types/#stringvector_1","title":"stringVector","text":"<p>Identical to <code>Rcpp::StringVector</code></p>"},{"location":"05-lesstimate_types/#use_r-0","title":"USE_R = 0","text":""},{"location":"05-lesstimate_types/#stringvector_2","title":"stringVector","text":"<p>Provides similar functionality as <code>Rcpp::StringVector</code>.</p>"},{"location":"05-lesstimate_types/#constructors","title":"Constructors:","text":"<ul> <li><code>stringVector() {}</code> </li> <li><code>stringVector(std::vector&lt;std::string&gt; values_) : values(values_) {}</code></li> </ul>"},{"location":"05-lesstimate_types/#fields","title":"Fields","text":"<ul> <li>field values: the values are a <code>std::vector&lt;std::string&gt;</code></li> </ul>"},{"location":"05-lesstimate_types/#methods","title":"Methods","text":"<p>at </p> <p>returns the element at a specific location of the string</p> <ul> <li>param where: location</li> <li>return auto&amp;</li> </ul> <p>size and length</p> <ul> <li>return returns the number of elements in a <code>stringVector</code></li> </ul> <p>fill</p> <p>Fill all elements of the <code>stringVector</code> with the same string</p> <ul> <li>param with: string to fill elements with</li> </ul>"},{"location":"05-lesstimate_types/#tostringvector","title":"toStringVector","text":"<p>cast <code>std::vector&lt;std::string&gt;</code> to <code>stringVector</code></p> <ul> <li>param vec: <code>std::vector&lt;std::string&gt;</code></li> <li>return stringVector</li> </ul>"},{"location":"05-lesstimate_types/#numericvector","title":"numericVector","text":"<p>numericVector provides functionality similar to Rcpp::NumericVector</p>"},{"location":"05-lesstimate_types/#constructor","title":"Constructor","text":"<ul> <li><code>numericVector()</code></li> <li><code>numericVector(int n_elem)</code></li> <li><code>numericVector(arma::rowvec vec)</code></li> <li><code>numericVector(arma::rowvec vec, std::vector&lt;std::string&gt; labels)</code></li> </ul>"},{"location":"05-lesstimate_types/#fields_1","title":"Fields","text":"<ul> <li>field values: <code>arma::rowvec</code></li> <li>field labels: labels for parameters</li> </ul>"},{"location":"05-lesstimate_types/#methods_1","title":"Methods","text":"<p>at </p> <p>returns the element at a specific location of the string</p> <ul> <li>param where: location</li> <li>return <code>auto&amp;</code></li> </ul> <p>()</p> <ul> <li>identical to at</li> </ul> <p>size and length</p> <ul> <li>return returns the number of elements in a stringVector</li> </ul> <p>fill</p> <p>Fill all elements of the stringVector with the same string</p> <ul> <li>param with: string to fill elements with</li> </ul> <p>names</p> <p>access the names of the <code>numericVector</code> elements</p> <ul> <li>return <code>stringVector&amp;</code></li> </ul>"},{"location":"05-lesstimate_types/#toarmavector","title":"toArmaVector","text":"<p>Cast <code>numericVector</code> to <code>arma::rowvec</code></p> <ul> <li>param numVec: <code>numericVector</code></li> <li>return <code>arma::rowvec</code></li> </ul>"},{"location":"05-lesstimate_types/#tonumericvector","title":"toNumericVector","text":"<p>Cast <code>arma::rowvec</code> to <code>numericVector</code></p> <ul> <li>param vec: <code>arma::rowvec</code></li> <li>return <code>numericVector</code></li> </ul>"},{"location":"06-Penalty%20type/","title":"Penalty type","text":"<p>Enum specifying the penalty type</p> <ul> <li>value none: no penalty</li> <li>value cappedL1: cappedL1 penalty</li> <li>value lasso: lasso penalty</li> <li>value lsp: lsp penalty</li> <li>value mcp: mcp penalty</li> <li>value scad: scad penalty</li> </ul>"},{"location":"07-GLMNET/","title":"GLMNET","text":"<p>The implementation of GLMNET follows that outlined in</p> <ol> <li>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1\u201320. https://doi.org/10.18637/jss.v033.i01</li> <li>Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., &amp; Lin, C.-J. (2010). A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification. Journal of Machine Learning Research, 11, 3183\u20133234.</li> <li>Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421</li> </ol>"},{"location":"07-GLMNET/#fitglmnet","title":"fitGlmnet","text":"<p>We provide two optimizer interfaces: One uses a combination of arma::rowvec and less::stringVector for starting values and parameter labels respectively. This interface is consistent with the fit and gradient function of the <code>less::model</code>-class. Alternatively, a numericVector can be passed to the optimizers. This design is rooted in the use of Rcpp::NumericVectors that combine values and labels similar to an R vector. Thus, interfacing to this second function call can be easier when coming from R.</p>"},{"location":"07-GLMNET/#version-1","title":"Version 1","text":"<ul> <li>param userModel: your model. Must inherit from less::model!</li> <li>param startingValues: numericVector with initial starting values. This vector can have names.</li> <li>param penalty: vector with strings indicating the penalty for each parameter. Currently supported are \"none\", \"cappedL1\", \"lasso\", \"lsp\", \"mcp\", and \"scad\". (e.g., {\"none\", \"scad\", \"scad\", \"lasso\", \"none\"}). If only one value is provided, the same penalty will be applied to every parameter!</li> <li>param lambda: lambda tuning parameter values. One lambda value for each parameter. If only one value is provided, this value will be applied to each parameter. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param theta: theta tuning parameter values. One theta value for each parameter If only one value is provided, this value will be applied to each parameter. Not all penalties use theta. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param initialHessian: matrix with initial Hessian values.</li> <li>param controlOptimizer: option to change the optimizer settings</li> <li>param verbose: should additional information be printed? If set &gt; 0, additional information will be provided. Highly recommended for initial runs. Note that the optimizer itself has a separate verbose argument that can be used to print information on each iteration. This can be set with the controlOptimizer - argument.</li> <li>return fitResults</li> </ul>"},{"location":"07-GLMNET/#version-2","title":"Version 2","text":"<ul> <li>param userModel: your model. Must inherit from less::model!</li> <li>param startingValues: an arma::rowvec numeric vector with starting values</li> <li>param parameterLabels: a less::stringVector with labels for parameters</li> <li>param penalty: vector with strings indicating the penalty for each parameter. Currently supported are \"none\", \"cappedL1\", \"lasso\", \"lsp\", \"mcp\", and \"scad\". (e.g., {\"none\", \"scad\", \"scad\", \"lasso\", \"none\"}). If only one value is provided, the same penalty will be applied to every parameter!</li> <li>param lambda: lambda tuning parameter values. One lambda value for each parameter. If only one value is provided, this value will be applied to each parameter. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param theta: theta tuning parameter values. One theta value for each parameter If only one value is provided, this value will be applied to each parameter. Not all penalties use theta. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param initialHessian: matrix with initial Hessian values.</li> <li>param controlOptimizer: option to change the optimizer settings</li> <li>param verbose: should additional information be printed? If set &gt; 0, additional information will be provided. Highly recommended for initial runs. Note that the optimizer itself has a separate verbose argument that can be used to print information on each iteration. This can be set with the controlOptimizer - argument.</li> <li>return fitResults</li> </ul>"},{"location":"07-GLMNET/#glmnet_1","title":"glmnet","text":"<p>Optimize a model using the glmnet procedure.</p>"},{"location":"07-GLMNET/#version-1_1","title":"Version 1","text":"<ul> <li>T-param nonsmoothPenalty: class of type nonsmoothPenalty (e.g., lasso, scad, lsp)</li> <li>T-param smoothPenalty: class of type smooth penalty (e.g., ridge)</li> <li>T-param tuning: tuning parameters used by both, the nonsmootPenalty and the smoothPenalty</li> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValuesRcpp: an Rcpp numeric vector with starting values</li> <li>param penalty_: a penalty derived from the penalty class in penalty.h</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the penalty functions. Note that both penalty functions must  take the same tuning parameters.</li> <li>param control_: settings for the glmnet optimizer.</li> <li>return fit result</li> </ul>"},{"location":"07-GLMNET/#version-2_1","title":"Version 2","text":"<ul> <li>T-param nonsmoothPenalty: class of type nonsmoothPenalty (e.g., lasso, scad, lsp)</li> <li>T-param smoothPenalty: class of type smooth penalty (e.g., ridge)</li> <li>T-param tuning: tuning parameters used by both, the nonsmootPenalty and the smoothPenalty</li> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValues: an arma::rowvec vector with starting values</li> <li>param parameterLabels: a stringVector with parameter labels</li> <li>param penalty_: a penalty derived from the penalty class in penalty.h</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the penalty functions. Note that both penalty functions must  take the same tuning parameters.</li> <li>param control_: settings for the glmnet optimizer.</li> <li>return fit result</li> </ul>"},{"location":"07-GLMNET/#convergencecriteriaglmnet","title":"convergenceCriteriaGlmnet","text":"<ul> <li>value GLMNET: Uses the convergence criterion outlined in Yuan et al. (2012) for GLMNET. Note that in case of BFGS, this will be identical to using the Armijo condition.</li> <li>value fitChange: Uses the change in fit from one iteration to the next.</li> <li>value gradients: Uses the gradients; if all are (close to) zero, the minimum is found</li> </ul>"},{"location":"07-GLMNET/#controldefaultglmnet","title":"controlDefaultGlmnet","text":"<p>Returns default for the optimizer settings</p>"},{"location":"07-GLMNET/#optimizer-settings","title":"Optimizer settings","text":"<p>The glmnet optimizer has the following additional settings:</p> <ul> <li><code>initialHessian</code>: an <code>arma::mat</code> with the initial Hessian matrix fo the optimizer. In case of the simplified interface, this  argument should not be used. Instead, pass the initial Hessian as shown above</li> <li><code>stepSize</code>: a <code>double</code> specifying the initial stepSize of the outer iteration ($\\theta_{k+1} = \\theta_k + \\text{stepSize} * \\text{stepDirection}$)</li> <li><code>sigma</code>: a <code>double</code> that is only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421.</li> <li><code>gamma</code>: a <code>double</code> controling the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.</li> <li><code>maxIterOut</code>: an <code>int</code> specifying the maximal number of outer iterations</li> <li><code>maxIterIn</code>: an <code>int</code> specifying the maximal number of inner iterations</li> <li><code>maxIterLine</code>: an <code>int</code> specifying the maximal number of iterations for the line search procedure</li> <li><code>breakOuter</code>: a <code>double</code> specyfing the stopping criterion for outer iterations</li> <li><code>breakInner</code>: a <code>double</code> specyfing the stopping criterion for inner iterations</li> <li><code>convergenceCriterion</code>: a <code>convergenceCriteriaGlmnet</code> specifying which convergence criterion should be used for the outer iterations. Possible are <code>less::GLMNET</code>, <code>less::fitChange</code>, and <code>less::gradients</code>. </li> <li><code>verbose</code>: an <code>int</code>, where 0 prints no additional information, &gt; 0 prints GLMNET iterations</li> </ul>"},{"location":"07-GLMNET/#penalties","title":"Penalties","text":""},{"location":"07-GLMNET/#cappedl1","title":"CappedL1","text":""},{"location":"07-GLMNET/#tuningparameterscappedl1glmnet","title":"tuningParametersCappedL1Glmnet","text":"<p>tuning parameters for the capped L1 penalty optimized with glmnet</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the cappedL1 penalty &gt; 0</li> </ul>"},{"location":"07-GLMNET/#penaltycappedl1glmnet","title":"penaltyCappedL1Glmnet","text":"<p>cappedL1 penalty for glmnet optimizer.</p> <p>The penalty function is given by: $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ where $\\theta &gt; 0$. The cappedL1 penalty is identical to the lasso for parameters which are below $\\theta$ and identical to a constant for parameters above $\\theta$. As adding a constant to the fitting function will not change its minimum, larger parameters can stay unregularized while smaller ones are set to zero.</p> <p>CappedL1 regularization:</p> <ul> <li>Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization. Journal of Machine Learning Research, 11, 1081\u20131107.</li> </ul>"},{"location":"07-GLMNET/#lasso","title":"lasso","text":""},{"location":"07-GLMNET/#tuningparametersenetglmnet","title":"tuningParametersEnetGlmnet","text":"<p>Tuning parameters of the elastic net. For glmnet, we allow for different alphas and lambdas to combine penalties.p</p> <ul> <li>param lambda: parameter-specific lambda value &gt;= 0</li> <li>param alpha: parameter-specific alpha value of the elastic net (relative importance of ridge and lasso)</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"07-GLMNET/#penaltylassoglmnet","title":"penaltyLASSOGlmnet","text":"<p>lasso penalty for glmnet</p> <p>The penalty function is given by: $$p( x_j) = \\lambda |x_j|$$ Lasso regularization will set parameters to zero if $\\lambda$ is large enough</p> <p>Lasso regularization:</p> <ul> <li>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267\u2013288.</li> </ul>"},{"location":"07-GLMNET/#lsp","title":"LSP","text":""},{"location":"07-GLMNET/#tuningparameterslspglmnet","title":"tuningParametersLspGlmnet","text":"<p>Tuning parameters for the lsp penalty optimized with glmnet</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the lsp penalty &gt; 0</li> </ul>"},{"location":"07-GLMNET/#penaltylspglmnet","title":"penaltyLSPGlmnet","text":"<p>Lsp penalty for glmnet optimizer.</p> <p>The penalty function is given by: $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ where $\\theta &gt; 0$.</p> <p>lsp regularization:</p> <ul> <li>Cand\u00e8s, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5\u20136), 877\u2013905. https://doi.org/10.1007/s00041-008-9045-x</li> </ul>"},{"location":"07-GLMNET/#mcp","title":"MCP","text":""},{"location":"07-GLMNET/#tuningparametersmcpglmnet","title":"tuningParametersMcpGlmnet","text":"<p>Tuning parameters for the mcp penalty optimized with glmnet</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the cappedL1 penalty &gt; 0</li> </ul>"},{"location":"07-GLMNET/#penaltymcpglmnet","title":"penaltyMcpGlmnet","text":"<p>Mcp penalty for glmnet optimizer</p> <p>The penalty function is given by: $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) &amp; \\text{if } |x_j| \\leq \\theta\\lambda\\ \\theta\\lambda^2/2 &amp; \\text{if } |x_j| &gt; \\lambda\\theta \\end{cases}$$ where $\\theta &gt; 1$.</p> <p>mcp regularization:</p> <ul> <li>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2), 894\u2013942. https://doi.org/10.1214/09-AOS729</li> </ul>"},{"location":"07-GLMNET/#mixed-penalty","title":"Mixed Penalty","text":""},{"location":"07-GLMNET/#tuningparametersmixedglmnet","title":"tuningParametersMixedGlmnet","text":"<p>Tuning parameters for the mixed penalty optimized with glmnet</p> <ul> <li>param penaltyType_: penaltyType-vector specifying the penalty to be used for each parameter</li> <li>param lambda: provide parameter-specific lambda values</li> <li>param theta: theta value of the mixed penalty &gt; 0</li> <li>param alpha: alpha value of the mixed penalty &gt; 0</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"07-GLMNET/#penaltymixedglmnet","title":"penaltyMixedGlmnet","text":"<p>Mixed penalty for glmnet optimizer</p>"},{"location":"07-GLMNET/#ridge","title":"Ridge","text":""},{"location":"07-GLMNET/#tuningparametersenetglmnet_1","title":"tuningParametersEnetGlmnet","text":"<p>Tuning parameters of the elastic net. For glmnet, we allow for different alphas and lambdas to combine penalties.p</p> <ul> <li>param lambda: parameter-specific lambda value &gt;= 0</li> <li>param alpha: parameter-specific alpha value of the elastic net (relative importance of ridge and lasso)</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"07-GLMNET/#penaltyridgeglmnet","title":"penaltyRidgeGlmnet","text":"<p>ridge penalty for glmnet optimizer</p> <p>The penalty function is given by: $$p( x_j) = \\lambda x_j^2$$ Note that ridge regularization will not set any of the parameters to zero but result in a shrinkage towards zero.</p> <p>Ridge regularization:</p> <ul> <li>Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55\u201367. https://doi.org/10.1080/00401706.1970.10488634</li> </ul>"},{"location":"07-GLMNET/#scad","title":"SCAD","text":""},{"location":"07-GLMNET/#tuningparametersscadglmnet","title":"tuningParametersScadGlmnet","text":"<p>Tuning parameters for the scad penalty optimized with glmnet</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the cappedL1 penalty &gt; 0</li> </ul>"},{"location":"07-GLMNET/#penaltyscadglmnet","title":"penaltySCADGlmnet","text":"<p>Scad penalty for glmnet</p> <p>The penalty function is given by: $$p( x_j) = \\begin{cases} \\lambda |x_j| &amp; \\text{if } |x_j| \\leq \\theta\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &amp; \\text{if } \\lambda &lt; |x_j| \\leq \\lambda\\theta \\ (\\theta + 1) \\lambda^2/2 &amp; \\text{if } |x_j| \\geq \\theta\\lambda\\ $$ where $\\theta &gt; 2$.</p> <p>scad regularization:</p> <ul> <li>Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456), 1348\u20131360. https://doi.org/10.1198/016214501753382273</li> </ul>"},{"location":"08-ISTA/","title":"ISTA","text":"<p>The implementation of ista follows that outlined in Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1), 183\u2013202. https://doi.org/10.1137/080716542 see Remark 3.1 on p. 191 (ISTA with backtracking)</p> <p>GIST can be found in Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013). A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems. Proceedings of the 30th International Conference on Machine Learning, 28(2)(2), 37\u201345.</p>"},{"location":"08-ISTA/#fitista","title":"fitIsta","text":"<p>We provide two optimizer interfaces: One uses a combination of arma::rowvec and less::stringVector for starting values and parameter labels respectively. This interface is consistent with the fit and gradient function of the <code>less::model</code>-class. Alternatively, a numericVector can be passed to the optimizers. This design is rooted in the use of Rcpp::NumericVectors that combine values and labels similar to an R vector. Thus, interfacing to this second function call can be easier when coming from R.</p>"},{"location":"08-ISTA/#version-1","title":"Version 1","text":"<ul> <li>param userModel: your model. Must inherit from less::model!</li> <li>param startingValues: numericVector with initial starting values. This  vector can have names.</li> <li>param penalty: vector with strings indicating the penalty for each parameter.  Currently supported are \"none\", \"cappedL1\", \"lasso\", \"lsp\", \"mcp\", and \"scad\".  (e.g., {\"none\", \"scad\", \"scad\", \"lasso\", \"none\"}). If only one value is provided,  the same penalty will be applied to every parameter!</li> <li>param lambda: lambda tuning parameter values. One lambda value for each parameter.  If only one value is provided, this value will be applied to each parameter.  Important: The the function will not loop over these values but assume that you  may want to provide different levels of regularization for each parameter!</li> <li>param theta: theta tuning parameter values. One theta value for each parameter  If only one value is provided, this value will be applied to each parameter.  Not all penalties use theta.  Important: The the function will not loop over these values but assume that you  may want to provide different levels of regularization for each parameter!</li> <li>param controlOptimizer: option to change the optimizer settings</li> <li>param verbose: should additional information be printed? If set &gt; 0, additional  information will be provided. Highly recommended for initial runs. Note that  the optimizer itself has a separate verbose argument that can be used to print  information on each iteration. This can be set with the controlOptimizer - argument.</li> <li>return fitResults</li> </ul>"},{"location":"08-ISTA/#version-2","title":"Version 2","text":"<ul> <li>param userModel: your model. Must inherit from less::model!</li> <li>param startingValues: an arma::rowvec numeric vector with starting values</li> <li>param parameterLabels: a less::stringVector with labels for parameters</li> <li>param penalty: vector with strings indicating the penalty for each parameter. Currently supported are \"none\", \"cappedL1\", \"lasso\", \"lsp\", \"mcp\", and \"scad\". (e.g., {\"none\", \"scad\", \"scad\", \"lasso\", \"none\"}). If only one value is provided, the same penalty will be applied to every parameter!</li> <li>param lambda: lambda tuning parameter values. One lambda value for each parameter. If only one value is provided, this value will be applied to each parameter. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param theta: theta tuning parameter values. One theta value for each parameter If only one value is provided, this value will be applied to each parameter. Not all penalties use theta. Important: The the function will not loop over these values but assume that you may want to provide different levels of regularization for each parameter!</li> <li>param controlOptimizer: option to change the optimizer settings</li> <li>param verbose: should additional information be printed? If set &gt; 0, additional information will be provided. Highly recommended for initial runs. Note that the optimizer itself has a separate verbose argument that can be used to print information on each iteration. This can be set with the controlOptimizer - argument.</li> <li>return fitResults</li> </ul>"},{"location":"08-ISTA/#ista_1","title":"ista","text":""},{"location":"08-ISTA/#version-1_1","title":"Version 1","text":"<p>Implements (variants of) the ista optimizer.</p> <ul> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValuesRcpp: an Rcpp numeric vector with starting values</li> <li>param proximalOperator_: a proximal operator for the penalty function</li> <li>param penalty_: a penalty derived from the penalty class in penalty.h</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the penalty function</li> <li>param smoothTuningParameters: tuning parameters for the smooth penalty function</li> <li>param control_: settings for the ista optimizer. </li> <li>return fit result</li> </ul>"},{"location":"08-ISTA/#version-2_1","title":"Version 2","text":"<p>Implements (variants of) the ista optimizer.</p> <ul> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValues: an arma::rowvec numeric vector with starting values</li> <li>param parameterLabels: a less::stringVector with labels for parameters</li> <li>param proximalOperator_: a proximal operator for the penalty function</li> <li>param penalty_: a penalty derived from the penalty class in penalty.h</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the penalty function</li> <li>param smoothTuningParameters: tuning parameters for the smooth penalty function</li> <li>param control_: settings for the ista optimizer.</li> <li>return fit result</li> </ul>"},{"location":"08-ISTA/#controldefault","title":"controlDefault","text":"<p>Returns default for the optimizer settings</p>"},{"location":"08-ISTA/#optimizer-settings","title":"Optimizer settings","text":"<p>The ista optimizer has the following additional settings:</p> <ul> <li><code>L0</code>: a <code>double</code> controling the step size used in the first iteration</li> <li><code>eta</code>: a <code>double</code> controling by how much the step size changes in inner iterations with $(\\eta^i)*L$, where $i$ is the inner iteration</li> <li><code>accelerate</code>: a <code>bool</code>; if  the extrapolation parameter is used to accelerate ista (see, e.g., Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms.  Foundations and Trends in Optimization, 1(3), 123\u2013231., p. 152)</li> <li><code>maxIterOut</code>: an <code>int</code> specifying the maximal number of outer iterations</li> <li><code>maxIterIn</code>: an <code>int</code> specifying the maximal number of inner iterations</li> <li><code>breakOuter</code>: a <code>double</code> specyfing the stopping criterion for outer iterations</li> <li><code>breakInner</code>: a <code>double</code> specyfing the stopping criterion for inner iterations</li> <li><code>convCritInner</code>: a <code>convCritInnerIsta</code> that specifies the inner breaking condition. Can be set to <code>less::istaCrit</code> (see Beck &amp; Teboulle (2009);  Remark 3.1 on p. 191 (ISTA with backtracking)) or <code>less::gistCrit</code> (see Gong et al., 2013; Equation 3) </li> <li><code>sigma</code>: a <code>double</code> in (0,1) that is used by the gist convergence criterion. Larger sigma enforce larger improvement in fit</li> <li><code>stepSizeIn</code>: a <code>stepSizeInheritance</code> that specifies how step sizes should be carried forward from iteration to iteration. <code>less::initial</code>: resets the step size to L0 in each iteration, <code>less::istaStepInheritance</code>: takes the previous step size as initial value for the next iteration, <code>less::barzilaiBorwein</code>: uses the Barzilai-Borwein procedure, <code>less::stochasticBarzilaiBorwein</code>: uses the Barzilai-Borwein procedure, but sometimes resets the step size; this can help when the optimizer is caught in a bad spot.</li> <li><code>sampleSize</code>: an <code>int</code> that can be used to scale the fitting function down if the fitting function depends on the sample size</li> <li><code>verbose</code>: an <code>int</code>, where 0 prints no additional information, &gt; 0 prints GLMNET iterations</li> </ul>"},{"location":"08-ISTA/#convcritinnerista","title":"convCritInnerIsta","text":"<p>Convergence criteria used by the ista optimizer.</p> <ul> <li>value istaCrit: The approximated fit based on the quadratic approximation h(parameters_k) := fit(parameters_k) + (parameters_k-parameters_kMinus1)gradients_k^T + (L/2)(parameters_k-parameters_kMinus1)^2 + penalty(parameters_k) is compared to the exact fit</li> <li>value gistCrit: the exact fit is compared to h(parameters_k) := fit(parameters_k) + penalty(parameters_kMinus1) + L(sigma/2)(parameters_k-parameters_kMinus1)^2</li> </ul>"},{"location":"08-ISTA/#stepsizeinheritance","title":"stepSizeInheritance","text":"<p>The ista optimizer provides different rules to be used to find an initial step size. It defines if and how the step size should be carried forward from iteration to iteration.</p> <ul> <li>value initial: resets the step size to L0 in each iteration</li> <li>value istaStepInheritance: takes the previous step size as initial value for the next iteration</li> <li>value barzilaiBorwein: uses the Barzilai-Borwein procedure</li> <li>value stochasticBarzilaiBorwein: uses the Barzilai-Borwein procedure, but sometimes resets the step size; this can help when the optimizer is caught in a bad spot.</li> </ul>"},{"location":"08-ISTA/#penalties","title":"Penalties","text":""},{"location":"08-ISTA/#cappedl1","title":"CappedL1","text":""},{"location":"08-ISTA/#tuningparameterscappedl1","title":"tuningParametersCappedL1","text":"<p>Tuning parameters for the cappedL1 penalty using ista</p> <ul> <li>param lambda: lambda value &gt;= 0</li> <li>param alpha: alpha value of the elastic net (relative importance of ridge and lasso)</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param theta: threshold parameter; any parameter above this threshold will only receive the constant penalty lambda_itheta, all below will get lambda_iparameterValue_i</li> </ul>"},{"location":"08-ISTA/#proximaloperatorcappedl1","title":"proximalOperatorCappedL1","text":"<p>Proximal operator for the cappedL1 penalty function</p>"},{"location":"08-ISTA/#penaltycappedl1","title":"penaltyCappedL1","text":"<p>CappedL1 penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ where $\\theta &gt; 0$. The cappedL1 penalty is identical to the lasso for parameters which are below $\\theta$ and identical to a constant for parameters above $\\theta$. As adding a constant to the fitting function will not change its minimum, larger parameters can stay unregularized while smaller ones are set to zero.</p> <p>CappedL1 regularization:</p> <ul> <li>Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization. Journal of Machine Learning Research, 11, 1081\u20131107.</li> </ul>"},{"location":"08-ISTA/#lasso","title":"lasso","text":""},{"location":"08-ISTA/#tuningparametersenet","title":"tuningParametersEnet","text":"<p>Tuning parameters for the lasso penalty using ista</p> <ul> <li>param lambda: lambda value &gt;= 0</li> <li>param alpha: alpha value of the elastic net (relative importance of ridge and lasso)</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"08-ISTA/#proximaloperatorlasso","title":"proximalOperatorLasso","text":"<p>Proximal operator for the lasso penalty function</p>"},{"location":"08-ISTA/#penaltylasso","title":"penaltyLASSO","text":"<p>Lasso penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\lambda |x_j|$$ Lasso regularization will set parameters to zero if $\\lambda$ is large enough</p> <p>Lasso regularization:</p> <ul> <li>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267\u2013288.</li> </ul>"},{"location":"08-ISTA/#lsp","title":"LSP","text":""},{"location":"08-ISTA/#tuningparameterslsp","title":"tuningParametersLSP","text":"<p>Tuning parameters for the lsp penalty using ista</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the lsp penalty &gt; 0</li> </ul>"},{"location":"08-ISTA/#proximaloperatorlsp","title":"proximalOperatorLSP","text":"<p>Proximal operator for the lsp penalty function</p>"},{"location":"08-ISTA/#penaltylsp","title":"penaltyLSP","text":"<p>Lsp penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ where $\\theta &gt; 0$.</p> <p>lsp regularization:</p> <ul> <li>Cand\u00e8s, E. J., Wakin, M. B., &amp; Boyd, S. P. (2008). Enhancing Sparsity by Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5\u20136), 877\u2013905. https://doi.org/10.1007/s00041-008-9045-x</li> </ul>"},{"location":"08-ISTA/#mcp","title":"MCP","text":""},{"location":"08-ISTA/#tuningparametersmcp","title":"tuningParametersMcp","text":"<p>Tuning parameters for the mcp penalty optimized with ista</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the cappedL1 penalty &gt; 0</li> </ul>"},{"location":"08-ISTA/#proximaloperatormcp","title":"proximalOperatorMcp","text":"<p>Proximal operator for the mcp penalty function</p>"},{"location":"08-ISTA/#penaltymcp","title":"penaltyMcp","text":"<p>Mcp penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) &amp; \\text{if } |x_j| \\leq \\theta\\lambda\\ \\theta\\lambda^2/2 &amp; \\text{if } |x_j| &gt; \\lambda\\theta \\end{cases}$$ where $\\theta &gt; 1$.</p> <p>mcp regularization:</p> <ul> <li>Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2), 894\u2013942. https://doi.org/10.1214/09-AOS729</li> </ul>"},{"location":"08-ISTA/#mixed-penalty","title":"Mixed penalty","text":""},{"location":"08-ISTA/#tuningparametersmixedpenalty","title":"tuningParametersMixedPenalty","text":"<p>Tuning parameters for the mixed penalty optimized with glmnet</p> <ul> <li>param penaltyType_: penaltyType-vector specifying the penalty to be used for each parameter</li> <li>param lambda: provide parameter-specific lambda values</li> <li>param theta: theta value of the mixed penalty &gt; 0</li> <li>param alpha: alpha value of the mixed penalty &gt; 0</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"08-ISTA/#proximaloperatormixedpenalty","title":"proximalOperatorMixedPenalty","text":"<p>Proximal operator for the mixed penalty function</p>"},{"location":"08-ISTA/#penaltymixedpenalty","title":"penaltyMixedPenalty","text":"<p>Mixed penalty</p>"},{"location":"08-ISTA/#ridge","title":"Ridge","text":""},{"location":"08-ISTA/#tuningparametersenet_1","title":"tuningParametersEnet","text":"<p>Tuning parameters for the lasso penalty using ista</p> <ul> <li>param lambda: lambda value &gt;= 0</li> <li>param alpha: alpha value of the elastic net (relative importance of ridge and lasso)</li> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> </ul>"},{"location":"08-ISTA/#penaltyridge","title":"penaltyRidge","text":"<p>Ridge penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\lambda x_j^2$$ Note that ridge regularization will not set any of the parameters to zero but result in a shrinkage towards zero.</p> <p>Ridge regularization:</p> <ul> <li>Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55\u201367. https://doi.org/10.1080/00401706.1970.10488634</li> </ul>"},{"location":"08-ISTA/#scad","title":"SCAD","text":""},{"location":"08-ISTA/#tuningparametersscad","title":"tuningParametersScad","text":"<p>Tuning parameters for the scad penalty optimized with ista</p> <ul> <li>param weights: provide parameter-specific weights (e.g., for adaptive lasso)</li> <li>param lambda: lambda value &gt;= 0</li> <li>param theta: theta value of the cappedL1 penalty &gt; 0</li> </ul>"},{"location":"08-ISTA/#proximaloperatorscad","title":"proximalOperatorScad","text":"<p>Proximal operator for the scad penalty function</p>"},{"location":"08-ISTA/#penaltyscad","title":"penaltyScad","text":"<p>Scad penalty for ista</p> <p>The penalty function is given by: $$p( x_j) = \\begin{cases} \\lambda |x_j| &amp; \\text{if } |x_j| \\leq \\theta\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &amp; \\text{if } \\lambda &lt; |x_j| \\leq \\lambda\\theta \\ (\\theta + 1) \\lambda^2/2 &amp; \\text{if } |x_j| \\geq \\theta\\lambda\\ $$ where $\\theta &gt; 2$.</p> <p>scad regularization:</p> <ul> <li>Fan, J., &amp; Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456), 1348\u20131360. https://doi.org/10.1198/016214501753382273</li> </ul>"},{"location":"09-fitResults/","title":"fitResults","text":"<p>All optimizers return a struct fitResults.</p> <ul> <li>value fit: the final fit value (regularized fit)</li> <li>value fits: a vector with all fits at the outer iteration</li> <li>value convergence: was the outer breaking condition met?</li> <li>value parameterValues: final parameter values</li> <li>value Hessian: final Hessian approximation (optional)</li> </ul>"},{"location":"10-BFGS/","title":"BFGS","text":""},{"location":"10-BFGS/#bfgsoptim","title":"bfgsOptim","text":"<p>Optimize a model using the BFGS optimizer. This optimizer does not support non-smooth penalty function (lasso, etc).</p>"},{"location":"10-BFGS/#version-1","title":"Version 1:","text":"<ul> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValuesRcpp: an Rcpp numeric vector with starting values</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the smoothPenalty function</li> <li>param control_: settings for the BFGS optimizer. Must be of struct controlBFGS. This can be created with controlBFGS.</li> <li>return fit result</li> </ul>"},{"location":"10-BFGS/#version-2","title":"Version 2","text":"<ul> <li>T-param T: type of the tuning parameters</li> <li>param model_: the model object derived from the model class in model.h</li> <li>param startingValues: an arma::rowvec numeric vector with starting values</li> <li>param parameterLabels: a lessSEM::stringVector with labels for parameters</li> <li>param smoothPenalty_: a smooth penalty derived from the smoothPenalty class in smoothPenalty.h</li> <li>param tuningParameters: tuning parameters for the smoothPenalty function</li> <li>param control_: settings for the BFGS optimizer. Must be of struct controlBFGS. This can be created with controlBFGS.</li> <li>return fit result</li> </ul>"},{"location":"10-BFGS/#controlbfgs","title":"controlBFGS","text":"<p>Struct that allows you to adapt the optimizer settings for the BFGS optimizer.</p> <ul> <li>param initialHessian: initial Hessian matrix fo the optimizer.</li> <li>param stepSize: Initial stepSize of the outer iteration (theta_{k+1} = theta_k + stepSize * Stepdirection)</li> <li>param sigma: only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https:*doi.org/10.1145/2020408.2020421.</li> <li>param gamma: Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999\u20132030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.</li> <li>param maxIterOut: Maximal number of outer iterations</li> <li>param maxIterIn: Maximal number of inner iterations</li> <li>param maxIterLine: Maximal number of iterations for the line search procedure</li> <li>param breakOuter: Stopping criterion for outer iterations</li> <li>param breakInner: Stopping criterion for inner iterations</li> <li>param convergenceCriterion: which convergence criterion should be used for the outer iterations? possible are 0 = GLMNET, 1 = fitChange, 2 = gradients.  Note that in case of gradients and GLMNET, we divide the gradients (and the Hessian) of the log-Likelihood by N as it would otherwise be  considerably more difficult for larger sample sizes to reach the convergence criteria.</li> <li>param verbose: 0 prints no additional information, &gt; 0 prints GLMNET iterations</li> </ul>"}]}